from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.sensors import SFTPSensor
from airflow.operators.bigquery_operator import BigQueryExecuteQueryOperator
from airflow.operators.email_operator import EmailOperator
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup
from google.cloud import bigquery, storage, secretmanager
import pandas as pd
import paramiko
import os
import json
import requests
from airflow.utils.email import send_email

# Helper function to get secrets from GCP Secret Manager
def get_secret(secret_name):
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/<your_project_id>/secrets/{secret_name}/versions/latest"
    response = client.access_secret_version(name=name)
    return response.payload.data.decode("UTF-8")

# Define constants for connection-related information
SFTP_CONN_ID = get_secret('your_sftp_connection')
BQ_RAW_TABLE = get_secret('your_bq_raw_table')
BQ_FINAL_TABLE = get_secret('your_bq_final_table')
TEAMS_WEBHOOK_URL = get_secret('your_teams_webhook_url')
EMAIL_RECIPIENTS = json.loads(get_secret('your_email_recipients'))

# Define constants for paths, folders, and bucket names
SFTP_PATH = os.getenv('SFTP_PATH')
LOCAL_FILE_PATH = os.getenv('LOCAL_FILE_PATH')
GCS_BUCKET = os.getenv('GCS_BUCKET')
GCS_LANDING_FOLDER = os.getenv('GCS_LANDING_FOLDER')
GCS_ARCHIVE_FOLDER = os.getenv('GCS_ARCHIVE_FOLDER')
GCS_YAML_CHECK_FOLDER = os.getenv('GCS_YAML_CHECK_FOLDER')

# Helper functions
def download_from_sftp(**kwargs):
    sftp_client = kwargs['sftp_client']
    sftp_client.get(SFTP_PATH, LOCAL_FILE_PATH)

def upload_to_gcs_landing(**kwargs):
    client = storage.Client()
    bucket = client.bucket(GCS_BUCKET)
    blob = bucket.blob(f'{GCS_LANDING_FOLDER}/{os.path.basename(LOCAL_FILE_PATH)}')
    blob.upload_from_filename(LOCAL_FILE_PATH)

def data_check(**kwargs):
    try:
        df = pd.read_csv(LOCAL_FILE_PATH, sep='\t')
        # Add more checks if necessary
    except Exception as e:
        raise ValueError(f"Data check failed: {e}")

def load_to_bq_raw(**kwargs):
    client = bigquery.Client()
    uri = f'gs://{GCS_BUCKET}/{GCS_LANDING_FOLDER}/{os.path.basename(LOCAL_FILE_PATH)}'
    table_id = BQ_RAW_TABLE
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,
    )
    load_job = client.load_table_from_uri(uri, table_id, job_config=job_config)
    load_job.result()

def get_unique_partition_dates(**kwargs):
    client = bigquery.Client()
    query = f"""
        SELECT DISTINCT DATE(partition_col) AS partition_date
        FROM `{BQ_RAW_TABLE}`
    """
    df = client.query(query).to_dataframe()
    return df['partition_date'].tolist()

def create_processing_task(partition_date):
    return BigQueryExecuteQueryOperator(
        task_id=f'process_data_{partition_date}',
        sql=f"""
            INSERT INTO `{BQ_FINAL_TABLE}` (employee_id, employee_name, partition_date)
            SELECT employee_id, employee_name, DATE(partition_col) AS partition_date
            FROM `{BQ_RAW_TABLE}`
            WHERE DATE(partition_col) = '{partition_date}'
        """,
        use_legacy_sql=False
    )

def send_teams_message(message):
    headers = {'Content-Type': 'application/json'}
    data = {'text': message}
    
